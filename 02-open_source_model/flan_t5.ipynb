{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"documents.json\", \"rt\") as f_in:\n",
    "    docs_raw = json.load(f_in)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6bbeff95d248eab22f744074ea0ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "## `Google-Flan-t5-xl`\n",
    "    # T5 Modeli\n",
    "        ## Text-To-Text Transfer Transformer: NLP görevlerini `text-to-text` formatında ele alır\n",
    "        ## Esneklik: Modeli çeşitli görevlerde (metin sınıflandırma, çeviri, özetleme, soru-cevap) esnek ve güçlü hale getirir.\n",
    "        ## Transfer Learning: Geniş bir veri kümesi üzerinde önceden eğitilir ve daha sonra belirli görevler için ince ayar yapılır.\n",
    "\n",
    "    # Flan: Modelin performansını artırmak için ek ince ayar ve eğitim adımlarını içerir.\n",
    "        ## Boyut: XL model, daha fazla parametre içerir ve bu da onun daha büyük ve daha güçlü bir model olmasını sağlar.\n",
    "        ## Gelişmiş Performans: Daha büyük boyutu ve ince ayar süreçleri sayesinde, Flan-T5 XL, dil anlama, metin üretimi ve diğer NLP görevlerinde oldukça yüksek performans gösterir.\n",
    "\n",
    "double_quant_config  = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\", quantization_config= double_quant_config)\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    prompt_template = \\\n",
    "    \"\"\"\n",
    "You are a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from th CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: {context}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def build_elasticsearch(index_settings, index_name, documents):\n",
    "    es_client = Elasticsearch('http://127.0.0.1:9200')\n",
    "\n",
    "    es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "    es_client.indices.create(index=index_name, body=index_settings)\n",
    "\n",
    "    for doc in tqdm(documents):\n",
    "        es_client.index(index=index_name, document=doc)\n",
    "\n",
    "    return es_client\n",
    "\n",
    "\n",
    "def es_search(es_client, search_query, index_name):\n",
    "    result_docs = []\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "\n",
    "    for hit in response['hits']['hits']:\n",
    "       result_docs.append(hit['_source'])\n",
    "\n",
    "    return result_docs\n",
    "\n",
    "\n",
    "def llm(prompt, generate_params=None):\n",
    "    if generate_params is None:\n",
    "        generate_params = {}\n",
    "\n",
    "## max_length = Daha uzun yanıtlar için, daha yüksek değerler ayarlayın.\n",
    "    ## num_beams = Bu değeri artırmak, olası dizilerin daha ayrıntılı olarak araştırılmasına yol açar. Tipik değerler 5 ile 10 arasındadır.\n",
    "    ## do_sample = Örnekleme yöntemlerini kullanmak için bunu True ayarlayın. Bu, daha çeşitli yanıtlar üretebilir.\n",
    "    ## temperature = Bu değeri düşürmek, modeli daha emin ve deterministik yapar, daha yüksek değerleri ise çeşitliliği artırır. Tipik değerler 0.7 ile 1.5 arasındadır.\n",
    "    ## top_k ve top_p = Bu parametreler çekirdek örneklemeyi kontrol eder. `top_k` örnekleme havuzunu en üst `k` token ile sınırlar, `top_p` ise kümülatif olasılığı kullanarak örnekleme havuzunu keser. Bu parametreleri istenen rastgelelik seviyesine göre ayarlayın.\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=generate_params.get(\"max_length\", 100),\n",
    "        num_beams=generate_params.get(\"num_beams\", 5),\n",
    "        do_sample=generate_params.get(\"do_sample\", True),\n",
    "        temperature=generate_params.get(\"temperature\", 0.5),\n",
    "        top_k=generate_params.get(\"top_k\", 1),\n",
    "        top_p=generate_params.get(\"top_p\", 0.95),)\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def rag(prompt):\n",
    "    answer = llm(prompt)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    }, \n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"elasticsearch-question\"\n",
    "\n",
    "query = \"I just discover the course. Can I still join it ?\"\n",
    "\n",
    "search_query = {\n",
    "  \"size\": 5,\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": {\n",
    "        \"multi_match\": {\n",
    "          \"query\": query,\n",
    "          \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "          \"type\": \"best_fields\"\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"term\": {\n",
    "          \"course\": \"data-engineering-zoomcamp\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e5f07601724ed797bb0ae7cbcb21b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "es_client = build_elasticsearch(index_settings, index_name, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = es_search(es_client, search_query, index_name)\n",
    "prompt = build_prompt(query, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad> Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.</s>\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-zoomcamp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
